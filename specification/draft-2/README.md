TODO: Intro

 - CWL specifies a way of describing runnable command line tools and workflows.
 - CWL descriptions are serialized as either YAML or JSON.
 - A CWL description contains a formal declaration of the inputs and outputs of described process (using Apache Avro schema).


# Core specification

CWL specification comes in two parts:

 - Core spec allows description of basic tools and workflows
 - Extensions add features that enable describing more complex tools and workflows

If using an extension feature, it needs to be explicitly listed in the requirements section
of the description document. Vendors might also add features incompatible with rest of CWL -
in this case, they should be explicitly listed as requirements with full URI as value of `class` property.


## Command line tools

Basic tool example:

```yaml
class: CommandLineTool
label: Grep
description: Filter lines of text file by regular expression.
inputs:
  - id: pattern
    type: string
    binding:
      position: 1
  - id: in_file
    type: File
    streamable: true
    binding:
      position: 2
outputs:
  - id: out_file
    type: File
    streamable: true
    binding:
      glob: output.txt
baseCmd: grep
stdout: output.txt
successCodes: [0, 1]
```

Above is defined a command line tool labeled "grep" with two required inputs
(a file and a string) and one file output.

Input objects contain a `binding` section which describes how the arguments
are passed onto the command line. In this case, `pattern` and `in_file` are
positional arguments and `pattern` is passed first, followed by `in_file`.
Identifiers for inputs and outputs must be unique across the document.

The `binding` section of the output object contains a single property `glob`
with a glob pattern describing how to recognize file generated by the tool.

Additionally, `baseCmd` and `stdout` properties describe the executable name
(optionally with additional hard-coded arguments like `python -m mymodule`)
and standard output redirection (into a file called "output.txt").

Success or failure of a job is determined by process exit code.
By default, 0 is success and everything else is failure.
However, some tools use exit codes to signal some information.
The grep tool will exit with code 1 if no lines matched the pattern.
For this purpose, we use the `successCodes` property to enumerate valid exit codes.

Values for the `type` property can be any valid Apache Avro schema (in JSON form)
with the `File` named type pre-defined. The `File` type declares that the input
value should be passed in form of a file reference. At runtime, the file object
itself will have at least the `path` and `size` properties, describing its
local path and size in bytes.

The File input and output of this tool are marked with the `streamable` flag to
indicate that the tool will not seek over these files. CWL implementations might
make use of this to pipe data from one tool to another.

### Running tools

A CWL implementation can run the above grep tool using a `Job` object like:

```yaml
inputs:
  in_file:
    __class: File
    path: /local/path/to/file.txt
    size: 42
  pattern: find ?me
allocatedResources:
  cpu: 1
  mem: 1024
```

The `inputs` property is a map that matches the input definitions of the tool.
Files are encoded as objects with `__class` property set to `File` and with
at least `path` and `size` properties.

Additionally, `allocatedResources` section describes that the tool has been
allocated a single CPU core and one GB of RAM.

Running the grep tool with this job will create the following command line:

```sh
grep "find ?me" /local/path/to/file.txt > output.txt
```

After running the tool (in a directory specifically created for the job),
resulting output object is created that looks like this:

```yaml
out_file:
  __class: File
  path: /path/to/job_directory/output.txt
  size: 13
```

### Optional and named arguments

Let's add some optional inputs to the grep tool: `--ignore-case` and `--context`

```yaml
# ...
inputs:
  # ...
  - id: ignore_case
    type: ['null', boolean]
    binding:
      prefix: --ignore-case
  - id: context
    type: ['null', integer]
    binding:
      prefix: --context
# ...
```

Above, type of these two inputs is declared as union of "null" (note this must be a string)
and their actual type to signify that these inputs are optional.

Bindings include the `prefix` property that specifies these should be passed to
command line as named arguments (or flags/switches). `position` property is not
declared, but it defaults to 0.

### Command line binding rules

Exact means of passing inputs to command line depend on the data type:

 - If value is `null` or `false`, input is ignored.
 - If value is `true`, only `prefix` is added at specified position.
 - If value is a `File` or a number, it is first converted to string.
 - If value is a string or converted to string, following rules are applied:
   - If no `prefix`, add value at specified position.
   - If no `separator`, add `prefix` and value as separate process arguments.
   - If `separator` is specified, add a single argument composed of `prefix`+`separator`+value.
 - If value is a list, add each item in order using these rules, unless
 - If value is a list with `itemSeparator` defined, convert each item to string and convert whole list to string using `itemSeparator`.join(elements_as_strings)
 - If value is an object (record), add individual fields using their own `binding` properties.

### Adapting tools without using bindings

One can adapt a command line tool to CWL without using the `binding` properties described above.

For every run, a job directory is created and the Job object is serialized into a file called
`job.cwl.json`. A wrapper script can load this file, run desired process, and serialize the outputs
(same as output object described above) into a file called `result.cwl.json`.

If this file exists after process finishes, CWL implementations will ignore `binding` properties
of output ports and simply load the contents of that file.

This method also allows command line tools to output values that are not files or lists of files,
but other types like integers or strings.

### Declaring tool requirements

You can declare runtime requirements of a tool using the `requirements` section.
Usually, this is used to specify the desired docker image and required CPU and memory:

```yaml
requirements:
  - class: DockerRequirement
    dockerRepo: boysha/tools#latest
    dockerImageID: im4g3h45h
  - class: CPURequirement
    value: 1  # Single-core process
  - class: MemoryRequirement
    value: 1024  # One GB
```

Sometimes, these are not hard requirements - rather, they serve as "hints" to optimize performance.
In these cases, simply place the requirement in the `hints` section instead.

A CWL implementation will not run anything with a requirement it does not recognize,
but may ignore the `hints` property.


## Describing workflows

Workflows in CWL are directed acyclical graphs (DAGs) of tools, with each edge annotated with
source and destination ports.

A workflow explicitly declares its own inputs and outputs in same manner as command line tools,
lists individual steps and encodes the wiring between them.

Workflow execution rules are simple: propagate data through the wiring (dataLinks),
run the steps with all dependencies resolved, repeat.

Let's look at a three-step workflow that uses the above grep tool and forwards its output
to a word count (`wc`) tool that counts the filtered lines:

```yaml
class: Workflow
inputs:
  - id: file_to_filter
    type: File
  - id: pattern
    type: string
outputs:
  - id: match_count
    type: integer
    connect:
      source: to_str/int

steps:

  - id: grep
    impl: grep.json
    inputs:
      - id: grep/in_file
        connect:
          source: file_to_filter
      - id: grep/pattern
        connect:
          source: pattern
    outputs:
      - id: grep/out_file

  - id: wc
    impl: wc.json
    inputs:
      - id: wc/in_file
        connect:
          source: grep/out_file
      - id: wc/count_lines
        defaultValue: true
    outputs:
      - id: wc/out_file

  - id: to_str
    impl: fileToString.json
    inputs:
      - id: to_str/in_file
        connect:
          source: wc/out_file
    outputs:
      - id: to_str/int
```

Each "step" re-declares relevant inputs and outputs so that they can be connected
or have input values hard-coded in the workflow (using the `defaultValue` property).

The re-declared ports have identifiers that follow the pattern of `<step_id>/<tool_port_id>`.
This needs to be done because individual tools might have same port identifiers or one can
use the same tool multiple times in the workflow.

The steps reference an external file with tool definition through their `impl` property.
This property need not be a string, it can be an object - the tool description itself, embedded.
However, if tools descriptions are embedded in this way, all relative identifiers inside the tool
must first be expanded to full URLs to preserve RDF compatibility (see RDF section).

Wiring of the workflow is encoded through the `connect` section, which usually just has the
`source` property whose value is the identifier of source port
(an output of previous step or input of the workflow).

The `source` property can also be an array of port identifiers, if one wishes to connect multiple
source ports to same destination. In this case, CWL implementations will create an array from all
incoming values. Example:

```yaml
steps:
  # ...
  - id: step2
    inputs:
      - id: step2/in_files
        connect:
          source: [step0/out_file, step1/out_file]
```



# Feature Extensions

CWL specifies additional features to support describing more complex tools and workflows.
Each of these features, if used, needs to be listed explicitly in the `requirements` section.


## Custom types

By requiring a `CustomTypesRequirement`, you can use the `schemaDefs` property
to enumerate named Avro types.

For example, a tool can define an input of complex type like this:

 ```yaml
 class: CommandLineTool
 requirements:
   - class: CustomTypesRequirement
 schemaDefs:
   - name: NamedGroupOfFiles
     type: record
     fields:
       - name: groupName
         type: string
       - name: fileList
         type:
           type: array
           items: File
inputs:
  - id: fileGroups
    type:
      type: array
      items: NamedGroupOfFiles
# ...
 ```

and the Job object might look like this:

```yaml
inputs:
  fileGroups:
    - groupName: group1
      fileList: [] # File objects listed here
    - groupName: group2
      fileList: [] # File objects listed here
```


## File metadata

By requiring `FileMetadataRequirement`, you can annotate produced files with key-value pairs
and attach "secondary" files that follow a naming convention to them (usually indices).

For example, a tool that creates an index for a BAM file can describe its output like this:

```yaml
class: CommandLineTool
label: bamtools index
requirements:
  - class: FileMetadataRequirement
  - class: ExpressionsRequirement
inputs:
  - id: bamFile
    type: File
    binding:
      position: 1
outputs:
  - id: indexed
    type: File
    binding:
      glob:
        class: Expression
        script: $job.inputs.bamFile.path
      fileMetadata:
        cwlm:format: BAM
      secondaryFiles: [.bai]
baseCmd: [bamtools, index]
```

The "bamtools index" command will create a .bai file in the same directory where BAM file is located.
The tool is described as taking a BAM file and producing an "indexed" BAM file, which is the same file
but with a .bai file "attached" to it. It also sets the metadata key `cwlm:format` to "BAM".

Metadata keys can be any strings, but we encourage use of URIs, specifically those in the cwlm namespace
(which is still not defined - work in progress).

The `secondaryFiles` property is a list of file extensions in form of `.ext` that get appended to
path of master file. This tells the CWL implementation to always make those files available in the
same directory as the master file. To replace the extension (rather than append) use `^.ext` -
e.g. `^.dict` for FASTA files.

Resulting output of this tool would look something like this:

```yaml
indexed:
  __class: File
  path: /path/to/file.bam
  size: 12345
  secondaryFiles: [.bai]
  metadata:
    cwlm:format: BAM
```

Note that the above tool has an "expression" object as value for the `glob` property.
This is needed because the output file is not in the job directory - it is in the same directory as input file.
To get the path of the input file, we used an embedded expression (see next section).


## Expressions

By requiring an `ExpressionsRequirement`, you are able to include objects like
`{"class": "Expression", "script": "javascript code here"}` to dynamically determine value of a property.

A `$job` variable will be made available to the script, holding the value of the Job object.
In input and output bindings, `$self` variable will also be set to the value of the input or path of
file matching the glob expression, respectively.

Value of the `script` property should be a javascript one-liner or a function body with `return`
statement, if code is enclosed with `{}`.

So, for example, `$job.inputs.an_int` and `{return $job.inputs.an_int}` are equivalent and both return
the value of `an_int` input.

### Non-input arguments and argument transforms

Sometimes, the value of an input will not be represented in same manner as expected by a tool.
For example, input might be a file with some secondary files (like reference.fasta and reference.dict)
but the tool will expect the "base name" of the file as input (e.g. just /path/to/reference, no extension).

In these cases, some string manipulation is needed before passing the input to command line.
For this, one can use the `argValue` property of input bindings. Set the value of this property to
an expression (which will have the `$self` variable set to the input file object) and it will be
evaluated before value is bound to the command line - the resulting value will be bound instead.

The case mentioned above could be handled like this:

```yaml
class: CommandLineTool
requirements:
  - class: ExpressionsRequirement
inputs:
  - id: ref
    type: File
    binding:
      position: 1
      argValue:
        class: Expression
        script: $self.path.split('.').slice(0, -1).join('.')
# ...
```

It is often the case that tools require arguments that map to allocated resources rather than
any specific input (e.g. number of threads or heap size).

For this purpose, one can use the `arguments` array where each item is similar to an input binding,
but it has no value in context so an expression must be used:

```yaml
class: CommandLineTool
requirements:
  - class: ExpressionsRequirement
# inputs and outputs not listed
baseCmd: [java, -jar, mytool.jar]
arguments:
  - prefix: -Xmx
    separator: ""
    argValue:
      class: Expression
      script: $job.allocatedResources.mem + 'M'
  - prefix: --num-threads
    position: 1
    argValue:
      class: Expression
      script: $job.allocatedResources.cpu
```

The above example would produce a command line like:
```sh
java -jar mytool.jar -Xmx512M --num-threads 3
```

### ExpressionTool component

At times, workflows might require a "shim" component - something that does not access file content
or perform any heavy calculations but simply slightly changes the shape/type of data.
For example, you may need to concatenate or zip two lists of files, or to convert a list of integers
to a list of strings.

For this purpose, you can use an "ExpressionTool" which is similar to CommandLineTools, but does not
have any bindings or command-line-specific things. It declares its inputs and outputs, but the implementation
is an expression embedded in the `runScript` property:

```yaml
class: ExpressionTool
label: ints to strings
description: Convert list of integers to list of strings.
inputs:
  - id: listOfInts
    type:
      type: array
      items: integer
outputs:
  - id: listOfStrings
    type:
      type: array
      items: string
runScript:
  script: |
    {
      var mapped = $job.inputs.listOfInts.map(function(i) {return i+''});
      return {listOfStrings: mapped};
    }
  class: Expression
```

### Creating configuration files

Some inputs may be passed to tools through configuration files.
You can create these files using the `createFiles` array.
Each item in this array is an object containing `filePath` and `fileContent` properties.
Usually, the `fileContent` property is an expression that creates the content for the
configuration file from the supplied Job, but it can be a string literal.

Example:

```yaml
class: CommandLineTool
requirements:
  - class: ExpressionsRequirement
createFiles:
  - filePath: ~/.cfg
    fileContent:
      class: Expression
      script: "SOME_VAR: " + $job.inputs.some_var
# ...
```

### Setting environment variables

Similarly, if some inputs map onto environment variables expected by the tool, use the
`environmentVars` array where items are objects with `envKey` and `envVal` properties:

```yaml
class: CommandLineTool
requirements:
  - class: ExpressionsRequirement
environmentVars:
  - envKey: SOME_VAR
    envVal:
      class: Expression
      script: job.inputs.some_var
# ...
```


## Nested workflows

Require the `NestedWorkflowsRequirement` if your workflow contains other workflows as components.
Nested workflows behave same as tools.


## Embedded tools

By requiring `EmbeddedToolsRequirement`, you can describe tools directly as workflow steps.
This way, inputs and outputs do not need to be re-declared, but note that you will not be able
to reuse these tools in other workflows without exctracting and modifying them:

```yaml
class: Workflow
requirements:
  - class: EmbeddedToolsRequirement
steps:
  - id: "step1"
    class: CommandLineTool
    inputs:
      - id: "in_file"
        type: File
        binding: {"position": 0}
        connect: {"source": "wf_input_file"}
    outputs:
      - id: "out_file"
        type: File
        binding:
          glob: output.txt
    stdout: output.txt
    baseCommand: "wc"
# ...
```

## Iteration in workflows

By requiring `ScatterRequirement` in the workflow, you can use the `scatter` property of
workflow steps to specify that the step should be executed for each item in the array supplied
to the listed port. In this case, the values on output ports will not match their specified type `T`
but will actually be of type `array<T>`.

For example:

```yaml
class: Workflow
requirements:
  - class: ScatterRequirement
inputs:
  - id: fastq_mates
    type:
      type: array
      items: File
outputs:
  - id: bam
    type: File
    connect: {"source": "align/bam"}
steps:
  - id: trim
    impl: trimFastq.json
    inputs:
      - id: trim/in_file
        connect: {"source": "fastq_mates"}
    outputs:
      - id: trim/out_file
    scatter: trim/in_file  # Run for each item on trim/in_file port
  - id: align
    impl: aligner.json
    inputs:
      - id: align/fq_mates
        connect: {"source": "trim/out_file"}  # Output of trim/out_file is a list due to scatter
    outputs:
      - id: align/bam
```

TODO: dot/cross product scatter, more detailed examples


## Expression engines

If you don't like javascript, or want to add a library of functions available in expressions,
you can create your own expression engine. You just need a docker image with an executable that
reads the expression (and some context) from stdin and writes result to stdout (in JSON).

For example, to add basic Python:

```yaml
requirements:
  - class: ExpressionEngineRequirement
    id: Python
    requirements:
      - class: DockerRequirement
        dockerPull: boysha/python_expressions_cwl_plugin
    engineCommand: python python_expression_plugin.py
stdin:
  class: Expression
  engine: Python  # Reference previously defined engine
  script: job['inputs']['in_file']['path']
```

Before running the tool, CWL implementations will create a docker container from `boysha/python_expression_plugin`
and run `python python_expression_plugin.py` process inside, with the following piped through stdin:

```json
{
  "job": {
    "inputs": {
      "in_file": {
        "__class": "File",
        "path": "/local/path/to/file.txt",
        "size": 42
      }
    },
    "allocatedResources": {"cpu": 1, "mem": 1024}
  },
  "context": null,
  "script": "job['inputs']['in_file']['path']"
}
```

The python_expression_plugin.py script `eval`s the passed `script` property with `job` variable set
and serializes the result on stdout as JSON.

Expression plugins must output valid JSON and exit with code 0, otherwise the tool run will be considered failed.


# JSON-LD and RDF compatibility

To make CWL tools and workflows valid RDF, simply add the following top-level property:

```yaml
"@context": "https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/schemas/draft-2/cwl-context.json"
```

This will make the document valid JSON-LD and parseable into RDF.
However, in-document identifiers and their references (every `id` property, like those of ports)
in the examples above are relative URLs. This might lead to conflicts - if the ID of our grep tool is
e.g. `http://example.com/grep`, the ID of its `in_file` port becomes `http://example.com/in_file`.

To handle this properly, port and other identifiers should be prefixed with `#` to make them document fragments.
Likewise, references to these (such as those in `connect` property) should also have the `#` prefix.
This is _only in the document itself_, not inside Job objects or expressions.

Tools and workflows following this rule are also valid [wfdesc](http://wf4ever.github.io/ro/#wfdesc) Processes.
